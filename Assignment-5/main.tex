\documentclass[journal,12pt,twocolumn]{IEEEtran}

\usepackage{setspace}
\usepackage{gensymb}
\singlespacing
\usepackage[cmex10]{amsmath}

\usepackage{amsthm}

\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{bm}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}

\usepackage{longtable}
\usepackage{multirow}

\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{steinmetz}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{verbatim}
\usepackage{tfrupee}
\usepackage[breaklinks=true]{hyperref}
\usepackage{graphicx}
\usepackage{tkz-euclide}

\usetikzlibrary{calc,math}
\usepackage{listings}
    \usepackage{color}                                            %%
    \usepackage{array}                                            %%
    \usepackage{longtable}                                        %%
    \usepackage{calc}                                             %%
    \usepackage{multirow}                                         %%
    \usepackage{hhline}                                           %%
    \usepackage{ifthen}                                           %%
    \usepackage{lscape}     
\usepackage{multicol}
\usepackage{chngcntr}

\DeclareMathOperator*{\Res}{Res}

\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}


\hyphenation{op-tical net-works semi-conduc-tor}
\def\inputGnumericTable{}                                 %%

\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}
\begin{document}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}

\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\bibliographystyle{IEEEtran}
\raggedbottom
\setlength{\parindent}{0pt}
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\newcommand*{\permcomb}[4][0mu]{{{}^{#3}\mkern#1#2_{#4}}}
\newcommand*{\perm}[1][-3mu]{\permcomb[#1]{P}}
\newcommand*{\comb}[1][-1mu]{\permcomb[#1]{C}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
%\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
%\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\newcommand{\cosec}{\,\text{cosec}\,}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\newcommand{\mydet}[1]{\ensuremath{}}
\numberwithin{equation}{subsection}
\makeatletter
\@addtoreset{figure}{problem}
\makeatother
\let\StandardTheFigure\thefigure
\let\vec\mathbf
\renewcommand{\thefigure}{\theproblem}
\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}
\vspace{3cm}
\title{Assignment 5}
\author{Vaibhav Chhabra \\ AI20BTECH11022}
\maketitle
\newpage
\bigskip
\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}
Download all latex codes from 
\begin{lstlisting}
https://github.com/vaibhavchhabra25/AI1103-course/blob/main/Assignment-5/main.tex
\end{lstlisting}
\section{Problem}
(UGC/MATH 2018 (June set-a)-Q.106) Let $\{X_i\}_{i \geq 1}$ be a sequence of i.i.d. random variables with $E(X_i)=0$ and $V(X_i)=1$. Which of the following are true?
\vspace{0.2cm}
\begin{enumerate}
    \item $\dfrac{1}{n} \sum_{i=1}^n X_i^2 \to 0$ in probability \vspace{0.2cm}
    \item $\dfrac{1}{n^{3/4}} \sum_{i=1}^n X_i \to 0$ in probability \vspace{0.2cm}
    \item $\dfrac{1}{n^{1/2}} \sum_{i=1}^n X_i \to 0$ in probability \vspace{0.2cm}
    \item $\dfrac{1}{n} \sum_{i=1}^n X_i^2 \to 1$ in probability
\end{enumerate}
\section{Solution}
\begin{definition}
    (Convergence in probability)\\
    Let $X1, X2$, . . . be an infinite sequence of random variables, and let $Y$ be another random variable. Then the sequence $\{X_n\}$ converges in probability to $Y$, if for all $\epsilon >0$, 
    \begin{align}
        \lim_{n \to \infty}\pr{|X_n-Y|\geq \epsilon}=0
    \end{align}
And we write as $n \to \infty$, $X_n \to Y$ in probability.
\end{definition}
\begin{enumerate}
    \item 
        \begin{theorem}
            (Strong Law of Large Numbers)\\
            Let $X_1, X_2, \cdot \cdot \cdot$ be a sequence of i.i.d. random variables, each having finite mean $E(X_i)$. Then for any $\epsilon >0$,
            \begin{align}
                \lim_{n \to \infty}\pr{\left|\dfrac{1}{n}\sum_{i=1}^nX_i - E(X_i)\right|\geq \epsilon}=0
            \end{align}
            Or, $\dfrac{1}{n}\sum_{i=1}^nX_i$ converges in probability to $E(X_i)$.
        \end{theorem}
        Given,
        \begin{align} 
            E(X_i)&=0 
            \label{a}\\
            V(X_i)&=1
            \label{b}
        \end{align}
        Also, we know that,
        \begin{align}
            E(X_i^2)=V(X_i)+(E(X_i))^2
        \end{align}
        Putting given values, we get,
        \begin{align} \label{c}
            E(X_i^2)&=1+0^2\\
            \implies E(X_i^2)&=1
        \end{align}
        So, $E(X_i^2)$ is finite.\\
        Let $F_{X_i}(x)$ be the c.d.f. for the random variable $X_i$.
        As $\{X_i\}$ is sequence of i.i.d. random variables, it follows the following conditions $\forall x,x_i \in \mathbb{R}$:
        \begin{enumerate}
            \item $F_{X_1}(x)=F_{X_2}(x)=\dots=F_{X_n}(x)=F(x) $
            \item $F_{X_1,X_2\dots X_n}(x_1,x_2\dots x_n)=F(x_1)F(x_2) \dots F(x_n)$
        \end{enumerate}
        Let $Y_i=X_i^2$. Then for $y \geq 0$,
        \begin{align}
            F_{Y_i}(y)&=\pr{Y_i \leq y}\\
            \implies F_{Y_i}(y)&=\pr{X_i^2 \leq y}\\
            \label{e}
            \implies F_{Y_i}(y)&=\pr{-\sqrt{y} \leq X_i \leq \sqrt{y}}\\
            \implies F_{Y_i}(y)&=\pr{X_i \leq \sqrt{y}}-\pr{X_i \leq -\sqrt{y}}\\
            \implies F_{Y_i}(y)&=F_{X_i}(\sqrt{y})-F_{X_i}(-\sqrt{y})
        \end{align}
        Using (a),
        \begin{align} \label{d}
            F_{Y_i}(y)&=F(\sqrt{y})-F(-\sqrt{y})
        \end{align}
        So, by \eqref{d},
        \begin{align} \label{f}
            F_{X_1^2}(y)=F_{X_2^2}(y)=\dots=F_{X_n^2}(y)=F_1(y)
        \end{align}
        where $F_1(y)$ is the c.d.f. of $Y_i=X_i^2$.\\
        Now, for $y_i\geq0$, consider
        \begin{align}
            &F_{Y_1,Y_2,\dots,Y_n}(y_1,y_2,\dots,y_n) \nonumber\\
            &=\pr{Y_1 \leq y_1,Y_2 \leq y_2,\dots,Y_n \leq y_n}
        \end{align}
        \begin{align}
            =&\pr{X_1^2 \leq y_1,X_2^2 \leq y_2,\dots,X_n^2 \leq y_n}\\
            =&\pr{-\sqrt{y_1} \leq X_1 \leq \sqrt{y_1},-\sqrt{y_2} \leq X_2 \leq \sqrt{y_2}, \nonumber\\
            &\dots,-\sqrt{y_n} \leq X_n \leq \sqrt{y_n}})
        \end{align}
        Since $X_1,X_2,\dots,X_n$ are independent,
        \begin{align}
            &F_{Y_1,Y_2,\dots,Y_n}(y_1,y_2,\dots,y_n)= \nonumber\\
            &\pr{-\sqrt{y_1} \leq X_1 \leq \sqrt{y_1}} \pr{-\sqrt{y_2} \leq X_2 \leq \sqrt{y_2}} \nonumber\\
            &\hspace{2cm} \dots \pr{-\sqrt{y_n} \leq X_n \leq \sqrt{y_n}}
        \end{align}
        Using \eqref{e} and \eqref{f},
        \begin{align}
            F_{Y_1,Y_2,\dots,Y_n}&(y_1,y_2,\dots,y_n) \nonumber \\
            &=F_{Y_1}(y_1) F_{Y_2}(y_2)\dots F_{Y_n}(y_n)\\
            &=F_1(y_1) F_1(y_2) \dots F_1(y_n)
        \end{align}
        So,
        \begin{align} \label{g}
            F_{X_1^2,X_2^2,\dots,X_n^2}&(y_1,y_2,\dots,y_n)\nonumber \\
            &=F_1(y_1) F_1(y_1)\dots F_1(y_n)
        \end{align}
        By \eqref{f} and \eqref{g}, $\{X_i^2\}$ must also be a sequence of i.i.d. random variables.\\
        So, we can apply S.L.L.N. to this sequence.\\
        Then, $\dfrac{1}{n}\sum_{i=1}^nX_i^2$ converges in probability to $E(X_i^2)$.\\
        Or, $\dfrac{1}{n}\sum_{i=1}^nX_i^2 \to 1$ in probability.\\
        Thus, option 1 is wrong.
    \item
        Now, we define
        \begin{align}
            Y_n=\dfrac{1}{n^{3/4}}\sum_{i=1}^nX_i
        \end{align}
        Then,
        \begin{align}
            E(Y_n) &= \dfrac{1}{n^{3/4}}E\brak{\sum_{i=1}^nX_i} \\
            \implies E(Y_n) &= \dfrac{1}{n^{3/4}}\brak{E(X_1)+E(X_2) \dots E(X_n)}
        \end{align}
        Using \eqref{a}
        \begin{align}
            E(Y_n) &= \dfrac{1}{n^{3/4}}\brak{0} =0
        \end{align}
        Now,
        \begin{align}
            V(Y_n) &= V\brak{\dfrac{1}{n^{3/4}}\sum_{i=1}^nX_i} \\
            \implies V(Y_n) &= \dfrac{1}{n^{3/2}}V\brak{X_1+X_2+ \dots +X_n}
        \end{align}
        As $X_1, X_2, \cdot \cdot \cdot X_n$ are independent of each other,
        \begin{align}
            V(Y_n) &= \dfrac{1}{n^{3/2}}\brak{V(X_1)+V(X_2)+ \dots +V(X_n)}
        \end{align}
        Using \eqref{b}
        \begin{align}
            V(Y_n)=\dfrac{1}{n^{3/2}}\brak{1+1+ \dots +1}=\dfrac{1}{n^{3/2}}\times n=\dfrac{1}{n^{1/2}}
        \end{align}
        Now for any $\epsilon>0$, consider the probability
        \begin{align}
            \pr{|Y_n-0|\geq \epsilon}=\pr{|Y_n-E(Y_n)|\geq \epsilon}
        \end{align}
        Applying Chebyschev's inequality here, we get,
        \begin{align}
            \pr{|Y_n-0|\geq \epsilon} \leq \dfrac{V(Y_n)}{\epsilon^2} = \dfrac{1}{n^{1/2}\epsilon^2}
        \end{align}
        So,
        \begin{align}
            \lim_{n \to \infty}\pr{|Y_n-0|\geq \epsilon} \leq \lim_{n \to \infty }\dfrac{1}{n^{1/2}\epsilon^2}=0\\
            \implies \lim_{n \to \infty}\pr{\left | \dfrac{1}{n^{3/4}}\sum_{i=1}^nX_i - 0 \right|\geq \epsilon}=0
        \end{align}
        So, $\dfrac{1}{n^{3/4}}\sum_{i=1}^nX_i \to 0$ in probability.\\
        Thus, option 2 is correct.
    \item
        \begin{definition}
        (Convergence in Distribution)\\
        Let $X, X_1, X_2,\dots $ be random variables. Then we say that the sequence $\{X_n\}$ converges to X, if $\forall x \in R^1$ such that $\pr{X=x}=0$, we have
        \begin{align}
            \lim_{n \to \infty}\pr{X_n \leq x}=\pr{X \leq x}.
        \end{align}
        \end{definition}
        \begin{theorem} \label{t-1}
        If $X_n \to X$ in probability, $X_n \to X$ in distribution.
        \end{theorem}
        \newpage
        \begin{theorem}
        (Central Limit Theorem)\\
        Let $X_1, X_2,\dots$ be i.i.d. random variables with finite mean $E(X_i)$ and finite variance $Var(X_i)$. Let $Z_n$ be defined as
        \begin{align}
            Z_n=\dfrac{\sum_{i=1}^nX_i-nE(X_i)}{(nVar(X_i))^{1/2}}
        \end{align}
         and let $Z \sim N(0,1)$.\\
         Then as $n \to \infty$, $Z_n \to Z$ in distribution.
        \end{theorem}
        Let
        \begin{align}
            Z_n=\dfrac{\sum_{i=1}^nX_i-nE(X_i)}{(nVar(X_i))^{1/2}}=\dfrac{1}{n^{1/2}}\sum_{i=1}^nX_i
        \end{align}
        And, let us assume that the given option is correct.
        Then, using theorem \ref{t-1}, $Z_n \to 0$ in probability $\implies Z_n \to 0$ in distribution.\\
        But by Central Limit Theorem, $Z_n \to Z$ in distribution, where $Z \sim N(0,1)$.
        So, it is a contradiction to our assumption and thus, option 3 must be wrong. 
    \item
        As proved for option (1), $\dfrac{1}{n}\sum_{i=1}^nX_i^2 \to 1$ in probability.
        So option 4 is correct.
\end{enumerate}
So the answer must be options 2,4.
\end{document}
